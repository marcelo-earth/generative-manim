{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Evaluation: Cross-Model x Cross-Stage Comparison\n",
    "\n",
    "Evaluate all 9 checkpoints (3 models x 3 stages) on 200 held-out test prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch transformers peft bitsandbytes accelerate manim rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline helpers (same as GRPO notebook)\n",
    "def extract_python_code(text):\n",
    "    match = re.findall(r'```python\\s*\\n(.*?)```', text, re.DOTALL)\n",
    "    if match: return match[0].strip()\n",
    "    match = re.findall(r'```\\s*\\n(.*?)```', text, re.DOTALL)\n",
    "    if match: return match[0].strip()\n",
    "    return text.strip()\n",
    "\n",
    "def clean_code(text):\n",
    "    code = extract_python_code(text)\n",
    "    match = re.search(r'class\\s+(\\w+)\\s*\\(.*Scene.*\\)', code)\n",
    "    if match and match.group(1) != 'GenScene':\n",
    "        code = code.replace(match.group(1), 'GenScene')\n",
    "    if 'from manim import' not in code:\n",
    "        code = 'from manim import *\\n' + code\n",
    "    return code\n",
    "\n",
    "def verify_code(code, timeout=60):\n",
    "    \"\"\"Returns (success, error_type, animation_count).\"\"\"\n",
    "    code = clean_code(code)\n",
    "    match = re.search(r'class\\s+(\\w+)\\s*\\(.*Scene.*\\)', code)\n",
    "    if not match:\n",
    "        return False, 'class_not_found', 0\n",
    "    \n",
    "    tmp_dir = tempfile.mkdtemp(prefix='eval_')\n",
    "    try:\n",
    "        path = os.path.join(tmp_dir, 'scene.py')\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(code)\n",
    "        result = subprocess.run(\n",
    "            ['manim', path, match.group(1), '--format=mp4', '-ql',\n",
    "             '--media_dir', tmp_dir, '--custom_folders'],\n",
    "            capture_output=True, text=True, timeout=timeout, cwd=tmp_dir\n",
    "        )\n",
    "        stderr = result.stderr or ''\n",
    "        anims = re.findall(r'Animation\\s+(\\d+):', stderr)\n",
    "        anim_count = max(int(a) for a in anims) + 1 if anims else 0\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            return True, 'none', anim_count\n",
    "        \n",
    "        if 'SyntaxError' in stderr: return False, 'syntax_error', 0\n",
    "        if 'ImportError' in stderr: return False, 'import_error', 0\n",
    "        if 'is not in the script' in stderr: return False, 'class_not_found', 0\n",
    "        if 'Traceback' in stderr: return False, 'runtime_error', 0\n",
    "        return False, 'unknown', 0\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return False, 'timeout', 0\n",
    "    except Exception as e:\n",
    "        return False, 'unknown', 0\n",
    "    finally:\n",
    "        shutil.rmtree(tmp_dir, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test prompts\n",
    "TEST_PATH = \"/kaggle/input/gm-training-data/test_prompts.jsonl\"\n",
    "prompts = []\n",
    "with open(TEST_PATH) as f:\n",
    "    for line in f:\n",
    "        prompts.append(json.loads(line)['prompt'])\n",
    "print(f'Test prompts: {len(prompts)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"Write Manim scripts for animations in Python. \"\n",
    "    \"Generate code, not text. Always use GenScene as the class name.\"\n",
    ")\n",
    "\n",
    "def evaluate_checkpoint(model_id, checkpoint_path, prompts, max_prompts=200):\n",
    "    \"\"\"Generate + evaluate for a single checkpoint.\"\"\"\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True\n",
    "    )\n",
    "    model = PeftModel.from_pretrained(base, checkpoint_path)\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    results = []\n",
    "    for prompt in tqdm(prompts[:max_prompts]):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs, max_new_tokens=2048, temperature=0.2,\n",
    "                do_sample=True, pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        response = tokenizer.decode(output[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        success, error_type, anims = verify_code(response)\n",
    "        results.append({'success': success, 'error': error_type, 'animations': anims})\n",
    "    \n",
    "    # Cleanup GPU\n",
    "    del model, base\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    successes = sum(1 for r in results if r['success'])\n",
    "    return {\n",
    "        'rate': successes / len(results),\n",
    "        'successes': successes,\n",
    "        'total': len(results),\n",
    "        'errors': Counter(r['error'] for r in results if not r['success']),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoints to evaluate\n",
    "# Adjust paths to your Kaggle datasets\n",
    "CHECKPOINTS = {\n",
    "    ('qwen2.5-coder-7b', 'sft'): ('/kaggle/input/gm-sft/sft-qwen2.5-coder-7b', 'Qwen/Qwen2.5-Coder-7B-Instruct'),\n",
    "    ('qwen2.5-coder-7b', 'dpo'): ('/kaggle/input/gm-dpo/dpo-qwen2.5-coder-7b', 'Qwen/Qwen2.5-Coder-7B-Instruct'),\n",
    "    ('qwen2.5-coder-7b', 'grpo'): ('/kaggle/input/gm-grpo/grpo-qwen2.5-coder-7b', 'Qwen/Qwen2.5-Coder-7B-Instruct'),\n",
    "    # Add other models as they become available\n",
    "}\n",
    "\n",
    "all_results = {}\n",
    "for (model_name, stage), (ckpt, model_id) in CHECKPOINTS.items():\n",
    "    if os.path.exists(ckpt):\n",
    "        print(f'\\nEvaluating {model_name}/{stage}...')\n",
    "        all_results[(model_name, stage)] = evaluate_checkpoint(model_id, ckpt, prompts)\n",
    "    else:\n",
    "        print(f'Skipping {model_name}/{stage} (checkpoint not found)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison table\n",
    "import pandas as pd\n",
    "\n",
    "models = ['qwen2.5-coder-7b', 'deepseek-coder-v2-lite', 'codellama-7b']\n",
    "stages = ['sft', 'dpo', 'grpo']\n",
    "\n",
    "data = []\n",
    "for model in models:\n",
    "    row = {'Model': model}\n",
    "    for stage in stages:\n",
    "        key = (model, stage)\n",
    "        if key in all_results:\n",
    "            r = all_results[key]\n",
    "            row[stage.upper()] = f\"{r['rate']:.0%} ({r['successes']}/{r['total']})\"\n",
    "        else:\n",
    "            row[stage.upper()] = '-'\n",
    "    data.append(row)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
