# SFT (Supervised Fine-Tuning) hyperparameters
sft:
  num_train_epochs: 3
  learning_rate: 2.0e-4
  lr_scheduler_type: cosine
  warmup_ratio: 0.03
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  fp16: true
  bf16: false
  logging_steps: 10
  save_steps: 100
  eval_steps: 100
  save_total_limit: 3
  max_grad_norm: 1.0
  weight_decay: 0.01
  optim: paged_adamw_32bit
  group_by_length: true
  report_to: wandb
  output_dir: ./outputs/sft
